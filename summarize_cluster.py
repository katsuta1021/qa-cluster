import time
from pathlib import Path
import pandas as pd
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer

# === 0. æº–å‚™ ===
project_dir = Path("/workspace/ELYZA_test")
output_dir = project_dir / "result"
output_dir.mkdir(exist_ok=True, parents=True)
output_path = output_dir / "elyza_summary_clusters.xlsx"
lecture_title = "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)è¬›åº§"

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
import sys
sys.path.append(str(project_dir.parent))
from common.data_loader import load_questions_df
df = load_questions_df()
df = df.dropna(subset=["question"]).copy()
questions = df["question"].tolist()

# === 1. ELYZA ãƒ¢ãƒ‡ãƒ«æº–å‚™ ===
model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# === 2. E5 ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ« ===
embedder = SentenceTransformer("intfloat/e5-large-v2", device=device)

def embed_texts(texts):
    return embedder.encode(
        [f"query: {s}" for s in texts],
        normalize_embeddings=True,
        batch_size=32,
        show_progress_bar=True
    )

def cluster_embeddings(embeddings):
    best_k, best_score = None, -1
    for k in range(8, 25, 2):
        km = KMeans(n_clusters=k, n_init="auto", random_state=42).fit(embeddings)
        score = silhouette_score(embeddings, km.labels_)
        if score > best_score:
            best_k, best_score = k, score
    kmeans = KMeans(n_clusters=best_k, n_init="auto", random_state=42).fit(embeddings)
    return kmeans.labels_, best_k, best_score

# === 3. ELYZAã«ã‚ˆã‚‹è¦ç´„é–¢æ•° ===
# ---- å®šæ•°ï¼ˆELYZA ç³»ï¼‰ 
B_INST, E_INST = "[INST]", "[/INST]"
B_SYS,  E_SYS  = "<<SYS>>\n", "\n<</SYS>>\n\n"
DEFAULT_SYSTEM_PROMPT = f"ã‚ãªãŸã¯è¬›ç¾©ã€Œ{lecture_title}ã€ã§ã®è³ªå•ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã‚„ã™ã„å½¢å¼ã«æ•´ãˆã‚‹ NLP ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚"
def normalize_question_with_elyza(text: str) -> dict:
    """
    å…ƒè³ªå•ã‚’æ­£è¦åŒ–ã—ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å‘ã‘ JSON ã‚’è¿”ã™ã€‚
    è¿”ã‚Šå€¤:
        {
            "canonical_question": str,   # 30 å­—ä»¥å†…
            "tags": [str, ...],          # 1ã€œ3 èª
            "status": "ready"|"needs_context"
        }
    """
    # ---- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬ä½“ ----
    system_instructions = """
- åŸæ–‡ã®æ—¥æœ¬èªå°‚é–€ç”¨èªã¯å¿…ãšæ®‹ã™
- ã‚ã„ã•ã¤ãƒ»è¬è¾ãªã©å›ç­”ã«ä¸è¦ãªèªå¥ã¯å‰Šé™¤ã™ã‚‹
- ãƒšãƒ¼ã‚¸ï¼å›³ï¼ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ã¯å…¨ã¦ <FIG> ã«ç½®æ›ã™ã‚‹
- canonical_question ã¯ 30 å­—ä»¥å†…ã§æ„å‘³æ ¸ã‚’æ®‹ã™
- <FIG> ãŒå«ã¾ã‚Œã‚‹å ´åˆ status ã‚’ "needs_context" ã¨ã— tags ã« "ref_figure" ã‚’è¿½åŠ 
- ãã‚Œä»¥å¤–ã¯ status ã‚’ "ready"
- tags ã¯ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯åè©ã‚’ 1ã€œ3 èªã€æ—¥æœ¬èªã§åˆ—æŒ™
å‡ºåŠ›ã¯ **ä»¥ä¸‹ JSON** ã®ã¿:
{"canonical_question": string,
 "tags": [string],
 "status": "ready" | "needs_context"}

 ä»¥ä¸‹ã®æ¡ä»¶ã‚’å³å¯†ã«å®ˆã£ã¦ãã ã•ã„ã€‚
- å‡ºåŠ›ã¯å¿…ãš JSON ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚
- JSON ã‚­ãƒ¼ã¯ "canonical_question", "tags", "status" ã®3ã¤ã ã‘ã§ã™ã€‚
- JSON ä»¥å¤–ã®æ–‡å­—ã¯çµ¶å¯¾ã«å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚
- canonical_question ã¯30æ–‡å­—ä»¥å†…ã€æ—¥æœ¬èªã®æ„å‘³æ ¸ã§ã‚ã‚‹ã“ã¨ã€‚
- tags ã¯1ã€œ3èªã®æ—¥æœ¬èªãƒˆãƒ”ãƒƒã‚¯åè©ï¼ˆä¾‹: "å‹¾é…é™ä¸‹æ³•", "å­¦ç¿’ç‡"ï¼‰ã€‚
- å…¥åŠ›æ–‡ã«å›³è¡¨ã‚„ãƒšãƒ¼ã‚¸æŒ‡å®šãŒå«ã¾ã‚Œã‚‹å ´åˆã€status ã¯ "needs_context" ã¨ã—ã€tags ã« "ref_figure" ã‚’è¿½åŠ ã€‚
- ä¸Šè¨˜ä»¥å¤–ã¯ status ã‚’ "ready" ã¨ã—ã¦ãã ã•ã„ã€‚
    """.strip()

    prompt_content = f"[ä¾é ¼]\n{text}\n[å›ç­”]"

    prompt = "{bos}{b_inst} {sys}{inst} {e_inst}".format(
        bos=tokenizer.bos_token,
        b_inst=B_INST,
        sys=f"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}",
        inst=f"{system_instructions}\n\n{prompt_content}",
        e_inst=E_INST,
    )

    input_ids = tokenizer.encode(
        prompt, return_tensors="pt", add_special_tokens=False
    ).to(device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            do_sample=False,
            max_new_tokens=320,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    raw_text = tokenizer.decode(
        output_ids[0][input_ids.shape[1]:],
        skip_special_tokens=False  # â† å¿µã®ãŸã‚ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚‚æ®‹ã™
    ).strip()

    result = {
        "_raw_text": raw_text  # â† ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãã®ã¾ã¾è¿”ã™
    }

    import json, re
    json_str = re.search(r"\{.*\}", raw_text, re.S)
    if not json_str:
        print("[WARNING] JSONå½¢å¼ã§å‡ºåŠ›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        return result

    try:
        result.update(json.loads(json_str.group(0)))
    except Exception as e:
        print("[ERROR] JSON decode failed:", e)
        print("å†…å®¹:", json_str.group(0))
    return result

# === 4. å®Ÿè¡Œ ===
print("=== ELYZAã«ã‚ˆã‚‹è¦ç´„é–‹å§‹ ===")
start = time.time()

raw_texts = [] 
norm_records = []
for q in tqdm(questions, desc="Normalizing"):
    result = normalize_question_with_elyza(q)
    norm_records.append(result)
    raw_texts.append(result.get("_raw_text", ""))  

df["canonical_question"] = [r.get("canonical_question", "") for r in norm_records]
df["tags"] = [", ".join(r.get("tags", [])) for r in norm_records]
df["status"] = [r.get("status", "") for r in norm_records]
df["elyza_raw"] = raw_texts

elapsed = time.time() - start
print(f"âœ… æ¨è«–æ™‚é–“: åˆè¨ˆ {elapsed:.2f}sã€å¹³å‡ {elapsed/len(questions):.2f}s")

# df["elyza_summary"] = summaries

# === 5. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ===
print("=== ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹ ===")
canonical_questions = [r.get("canonical_question", "") for r in norm_records]
emb = embed_texts(canonical_questions)  
labels, best_k, best_s = cluster_embeddings(emb)
df["elyza_cluster"] = labels
print(f"âœ… ã‚¯ãƒ©ã‚¹ã‚¿æ•°: K={best_k}, silhouette={best_s:.3f}")

# === 6. ä¿å­˜ ===
df.to_excel(output_path, index=False)
print(f"ğŸ“„ å‡ºåŠ›å®Œäº†: {output_path}")
